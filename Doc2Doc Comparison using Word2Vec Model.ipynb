{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Doc Comparison using Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre tensorflow, TF-IDF was a popular model of choice which relied primarily on exact word/synonym matching across documents. Main drawback of this approach is it loses context information (no word-word relationship).\n",
    "\n",
    "Word2Vec model improves upon it by assigning a “vector” to each word in the vocabulary. Similar meaning words tend to have vectors in the same “neighborhood” in a geometrical sense.\n",
    "\n",
    "In this notebook, we identify the similarity among different documents based on the Word2Vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This program is based on Basic word2vec example.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "from six.moves import xrange\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "from os import listdir\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define file path and user inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a folder path as an argument with '--log_dir' to save\n",
    "# TensorBoard summaries. Default is a log folder in current directory.\n",
    "current_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--log_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'log'),\n",
    "    help='The log directory for TensorBoard summaries.')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# Create the directory for TensorBoard variables if there is not.\n",
    "if not os.path.exists(FLAGS.log_dir):\n",
    "    os.makedirs(FLAGS.log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Read the data into a list of strings.\n",
    "    \n",
    "    Parameters:\n",
    "        filename: file name of the data to be read\n",
    "    Return:\n",
    "        data: a list of strings\n",
    "              (ex.\n",
    "                  original data: I love coding\n",
    "                  data: ['I', 'love', 'coding'])\n",
    "    \"\"\"\n",
    "    tot_lines = ''\n",
    "    infile = codecs.open(filename, 'r', 'utf-8')\n",
    "    inlines = infile.readlines()\n",
    "    for line in inlines:\n",
    "        tot_lines += line + \" \"\n",
    "    data = tot_lines.split()\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"\n",
    "    Process raw inputs into a dataset.\n",
    "    Parameters:\n",
    "        words: data which we have read\n",
    "        n_words: length of data\n",
    "    Return:\n",
    "        data: list of codes (integers from 0 to vocabulary_size-1).\n",
    "              This is the original text but words are replaced by their codes\n",
    "        count: map of words(strings) to count of occurrences\n",
    "        dictionar: map of words(strings) to their codes(integers)\n",
    "        reversed_dictionary: maps codes(integers) to words(strings)\n",
    "    \"\"\"\n",
    "    count = []\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0) # 0 is a default value to return if the key doesn’t exist.\n",
    "        data.append(index)\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = read_data('clean_data.txt')\n",
    "print('Data size', len(vocabulary))\n",
    "word_count = len(vocabulary)\n",
    "\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary, word_count)\n",
    "del vocabulary  # Hint to reduce memory.\n",
    "print('Most common words', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0\n",
    "vocabulary_size = len(count)\n",
    "\n",
    "print(\"Unique word count: \" + str(len(count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training batches for the skip-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/get_batch.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    \"\"\"\n",
    "    Generate batches\n",
    "    Parameters:\n",
    "        batch_size: batch size\n",
    "        num_skips: How many times to reuse an input to generate a label\n",
    "        skip_window: window size. How many words to consider left and right\n",
    "    Return:\n",
    "        batch: batches of data\n",
    "        labels:  batches of labels\n",
    "    \"\"\"\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two architectures for implementing word2vec, CBOW(Contineous Bag-Of-Words) and Skip-gram[(PDF)](https://arxiv.org/pdf/1301.3781.pdf). In this project, we use the skip-gram to train the network to learn representation for words that show up in similar contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/architectures_for_word2vec.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity will generate a metric that says how related are two words by looking at the angle. So, similar words have “cosine similarity” near +1 and dissimilar words have “cosine similarity” near -1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/cosine_similarity.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are just like a fully connected layer. we can skip complicated computation into the embedding layer by instead directly grabbing the hidden layer values from the weight matrix.\n",
    "\n",
    "Take word \"heart\" as an example, it is encoded as 10. We can use the weight matrix as lookup table, that is, we just take the 10th row of the embedding matrix and get the word vector for the word, heart. Therefore, we don't need to do any matrix manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/embeddings.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "all_word_idxs = np.arange(0, vocabulary_size)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('inputs'):\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\t\n",
    "    all_word_dataset = tf.constant(all_word_idxs, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "      # Look up embeddings for inputs.\n",
    "        with tf.name_scope('embeddings'):\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "        with tf.name_scope('weights'):\n",
    "            nce_weights = tf.Variable(\n",
    "                tf.truncated_normal(\n",
    "                    [vocabulary_size, embedding_size],\n",
    "                    stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        with tf.name_scope('biases'):\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        # Explanation of the meaning of NCE loss:\n",
    "        #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.reduce_mean(\n",
    "              tf.nn.nce_loss(\n",
    "                  weights=nce_weights,\n",
    "                  biases=nce_biases,\n",
    "                  labels=train_labels,\n",
    "                  inputs=embed,\n",
    "                  num_sampled=num_sampled,\n",
    "                  num_classes=vocabulary_size))\n",
    "\n",
    "        # Add the loss value as a scalar to summary.\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "        # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "        with tf.name_scope('optimizer'):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "        # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        \n",
    "        # for validation dataset\n",
    "        valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)\n",
    "        similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "        # for training dataset\n",
    "        all_embeddings = tf.nn.embedding_lookup(normalized_embeddings, all_word_dataset)\n",
    "        all_similarity = tf.matmul(all_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "        distance_matrix = tf.placeholder(tf.float32, shape=(None,None,None))\n",
    "        doc_to_doc_similarities = tf.reduce_sum(tf.reduce_max(distance_matrix, axis=2), axis=1)\n",
    "\n",
    "        # Merge all summaries.\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Create a saver.\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d1_d2_dist_array_3d tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/d1_d2_dist_array_3d.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100000\n",
    "\n",
    "def get_document_names(base_dir):\n",
    "\tdoc_names = []\n",
    "\tfor f in listdir(base_dir):\n",
    "\t\tfname = str(f)\n",
    "\t\tdoc_names.append(fname)\n",
    "\treturn doc_names\n",
    "\n",
    "doc_names = get_document_names('clean_files')\n",
    "num_docs = int(sys.argv[1])\n",
    "if len(doc_names) < num_docs:\n",
    "    num_docs = len(doc_names)\n",
    "\n",
    "print('Number of documents: ', num_docs)\n",
    "\n",
    "doc_words = {}\n",
    "for doc in doc_names:\n",
    "    infile = codecs.open(os.path.join('clean_files', doc), 'r', 'utf-8')\n",
    "    inlines = infile.readlines()\n",
    "    tot_lines = ''\n",
    "    for line in inlines:\n",
    "        tot_lines += line + \" \"\n",
    "    toks = tot_lines.split()\n",
    "\n",
    "    most_common_count = int(sys.argv[2])\n",
    "    print('Number of top occuring words in each document: ', most_common_count)\n",
    "    count = []\n",
    "    count.extend(collections.Counter(toks).most_common(most_common_count))\n",
    "\n",
    "    widx_list = []\n",
    "    for tok, _ in count:\n",
    "        if tok in dictionary.keys():\n",
    "            tok_widx = dictionary[tok]\n",
    "            if tok_widx not in widx_list:\n",
    "                widx_list.append(tok_widx)\n",
    "    doc_words[doc] = widx_list\n",
    "    widx_array = np.asarray(widx_list, 'int32')\n",
    "    doc_words[doc] = widx_array\n",
    "\n",
    "doc_similarities = np.zeros(shape=(num_docs, num_docs), dtype='float32')\n",
    "d1_d2_dist_array_3d = np.zeros(shape=(num_docs,most_common_count,most_common_count), dtype='float32')\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter(FLAGS.log_dir, session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    #################################################\n",
    "    #\n",
    "    # This section trains the model to word vectors\n",
    "    #\n",
    "    #################################################\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "        # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run(\n",
    "                               [optimizer, merged, loss],\n",
    "                               feed_dict=feed_dict,\n",
    "                               run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        # Add returned summaries to writer in each step.\n",
    "        writer.add_summary(summary, step)\n",
    "        # Add metadata to visualize the graph for the last run.\n",
    "        if step == (num_steps - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 10000 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "        final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the distance matrix across vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_word_similarities matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/word_word_similarity_matrix.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the word to word similarities among all words we have passed into the model in above matrix for later document comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_similarity_val = all_similarity.eval() \n",
    "print(all_similarity_val.shape)\n",
    "word_word_idxs = np.zeros(shape=(vocabulary_size,vocabulary_size), dtype='int32')\n",
    "word_word_similarities = np.zeros(shape=(vocabulary_size, vocabulary_size), dtype='float32')\n",
    "for rowIdx in xrange(vocabulary_size):\n",
    "    dist_row = all_similarity_val[rowIdx]\n",
    "    word_word_idxs[rowIdx,] = dist_row\n",
    "    word_word_similarities[rowIdx,] = dist_row # get all the word to word similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the documents with each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare all the words in each document respectively and then choose the maximum value of similarity for each word in a document to all the words in another document. Finally, add those value of similarity together to get the document to document similarity.\n",
    "\n",
    "See below example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/doc2doc_similarity_calc.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare every single word in doc1 to all words in doc2. Then we get all the maximun value which are highlighted in red for each comapred word-pair. Then add those maximun value toghter to get the Doc2Doc similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_doc_size = 0\n",
    "for docIdx in range(num_docs):\n",
    "    doc_name = doc_names[docIdx]\n",
    "    doc1_widxs =  doc_words[doc_name] # doc_words is a dictionary (key, value) = {doc_name, widx of the doc}\n",
    "    if (len(doc1_widxs) > max_doc_size):\n",
    "        max_doc_size = len(doc1_widxs)\n",
    "\n",
    "for docIdx in range(num_docs):\n",
    "    d1_d2_dist_array_3d.fill(0)\n",
    "    doc_name = doc_names[docIdx]\n",
    "    doc1_widxs = doc_words[doc_name]\n",
    "    for nextIdx in range(num_docs):\n",
    "        next_name = doc_names[nextIdx]\n",
    "        doc2_widxs = doc_words[next_name]\n",
    "\n",
    "        rows = len(doc1_widxs)\n",
    "        cols = len(doc2_widxs)\n",
    "\n",
    "        for ridx in range(rows):\n",
    "            d1_d2_row = np.zeros(shape=(max_doc_size), dtype='float32')\n",
    "            doc1_word_to_vocabulary_values = word_word_similarities[doc1_widxs[ridx],]\n",
    "            d1_d2_row[0:cols] = doc1_word_to_vocabulary_values[doc2_widxs]\n",
    "            doc1_word_to_all_doc2_words_dists = d1_d2_row\n",
    "            d1_d2_dist_array_3d[nextIdx,ridx,] = doc1_word_to_all_doc2_words_dists\n",
    "\n",
    "    d1_d2_similarities = doc_to_doc_similarities.eval(feed_dict={distance_matrix: d1_d2_dist_array_3d})\n",
    "    doc_similarities[docIdx,] = d1_d2_similarities\n",
    "\n",
    "\n",
    "nearest_doc_idxs = np.zeros((num_docs, num_docs), dtype='int32')\n",
    "for rowIdx in xrange(num_docs):\n",
    "    similarity_row = doc_similarities[rowIdx]\n",
    "    sorted_negative_similarity_row_idxs = (-similarity_row).argsort()\n",
    "    nearest_doc_idxs[rowIdx,] = sorted_negative_similarity_row_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the top 10 documents that are closest to each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('doc_comparisons.txt', 'w')\n",
    "    for docIdx in range(num_docs):\n",
    "        doc_name = doc_names[docIdx]\n",
    "        nearest_list = []\n",
    "        for i in range(1,11):\n",
    "            nearest_doc_idx = nearest_doc_idxs[docIdx,i]\n",
    "            tmp_name = doc_names[nearest_doc_idx]\n",
    "            tmp_similarity = doc_similarities[docIdx,nearest_doc_idx]\n",
    "            nearest_list.append((tmp_name, tmp_similarity))\n",
    "        outfile.write(doc_name + \" => \" + str(nearest_list) + \"\\n\")\n",
    "    outfile.close()\n",
    "\n",
    "    \n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join(FLAGS.log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw visualization of distance between embeddings.\n",
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y),\n",
    "            xytext=(5, 2),\n",
    "            textcoords='offset points',\n",
    "            ha='right',\n",
    "            va='bottom')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(), 'tsne.png'))\n",
    "\n",
    "except ImportError as ex:\n",
    "    print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/embeddings_output.png'>\n",
    "<img src='assets/loss_graph.png'>\n",
    "<img src='assets/top10_output.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
